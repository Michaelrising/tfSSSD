import tensorflow as tf
from tensorflow import keras


class BaseAttention(keras.layers.Layer):
  def __init__(self, **kwargs):
    super().__init__()
    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)
    self.layernorm = tf.keras.layers.LayerNormalization()
    self.add = tf.keras.layers.Add()


class CrossAttention(BaseAttention):
  def call(self, x, context):
    attn_output, attn_scores = self.mha(
        query=x,
        key=context,
        value=context,
        return_attention_scores=True)

    # Cache the attention scores for plotting later.
    self.last_attn_scores = attn_scores

    x = self.add([x, attn_output])
    x = self.layernorm(x)

    return x


class GlobalSelfAttention(BaseAttention):
  def call(self, x):
    attn_output = self.mha(
        query=x,
        value=x,
        key=x)
    x = self.add([x, attn_output])
    x = self.layernorm(x)
    return x


class CausalSelfAttention(BaseAttention):
  def call(self, x):
    attn_output = self.mha(
        query=x,
        value=x,
        key=x,
        use_causal_mask = True)
    x = self.add([x, attn_output])
    x = self.layernorm(x)
    return x


class FeedForward(tf.keras.layers.Layer):
  def __init__(self, d_model, dff, dropout_rate=0.1):
    super().__init__()
    self.seq = tf.keras.Sequential([
      tf.keras.layers.Dense(dff, activation='relu'),
      tf.keras.layers.Dense(d_model),
      tf.keras.layers.Dropout(dropout_rate)
    ])
    self.add = tf.keras.layers.Add()
    self.layer_norm = tf.keras.layers.LayerNormalization()

  def call(self, x):
    x = self.add([x, self.seq(x)])
    x = self.layer_norm(x)
    return x


class EncoderLayer(tf.keras.layers.Layer):
  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):
    super().__init__()

    self.self_attention = GlobalSelfAttention(
        num_heads=num_heads,
        key_dim=d_model,
        dropout=dropout_rate)

    self.ffn = FeedForward(d_model, dff)

  def call(self, x):
    x = self.self_attention.call(x)
    x = self.ffn.call(x)
    return x

  class Encoder(tf.keras.layers.Layer):
      def __init__(self, *, num_layers, d_model, num_heads,
                   dff, vocab_size, dropout_rate=0.1):
          super().__init__()

          self.d_model = d_model
          self.num_layers = num_layers

          self.enc_layers = [
              EncoderLayer(d_model=d_model,
                           num_heads=num_heads,
                           dff=dff,
                           dropout_rate=dropout_rate)
              for _ in range(num_layers)]
          self.dropout = tf.keras.layers.Dropout(dropout_rate)

      def call(self, x):
          # `x` shape: (batch, seq_len, feature)
          # Add dropout.
          x = self.dropout(x)

          for encoder in self.enc_layers:
              x = encoder.call(x)

          return x  # Shape `(batch_size, seq_len, d_model)`.